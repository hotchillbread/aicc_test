import asyncio
import time
import logging
import os
from typing import Dict, Any, Optional
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    print("âš ï¸ openaiê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    OPENAI_AVAILABLE = False

# í•œêµ­ì–´ LLM (HuggingFace)
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
    import torch
    HUGGINGFACE_AVAILABLE = True
except ImportError:
    print("âš ï¸ transformers/torchê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
    HUGGINGFACE_AVAILABLE = False

logger = logging.getLogger(__name__)

class LLMService:
    def __init__(self):
        self.openai_client = None
        self.korean_llm = None
        self.korean_tokenizer = None
        self.korean_model = None
        self.current_korean_model = None  # í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ëª…
        self.device = "cpu"  # CPU ì‚¬ìš©
        
    async def initialize(self):
        """LLM ëª¨ë¸ë“¤ ì´ˆê¸°í™”"""
        logger.info("LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        # OpenAI ì´ˆê¸°í™”
        if OPENAI_AVAILABLE:
            try:
                # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸°
                api_key = os.getenv("OPENAI_API_KEY")
                if api_key:
                    self.openai_client = openai.OpenAI(api_key=api_key)
                    logger.info("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    logger.warning("âš ï¸ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            except Exception as e:
                logger.error(f"âŒ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        
        # í•œêµ­ì–´ LLM ì´ˆê¸°í™”
        if HUGGINGFACE_AVAILABLE:
            try:
                logger.info("ğŸ‡°ğŸ‡· í•œêµ­ì–´ LLM ëª¨ë¸ ë¡œë“œ ì¤‘...")
                
                # í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ë“¤ ìš°ì„ ìˆœìœ„ë³„ë¡œ ì‹œë„
                korean_models = [
                    "microsoft/DialoGPT-small"            # í•œêµ­ì–´ BERT
                ]
                
                for model_name in korean_models:
                    try:
                        logger.info(f"ëª¨ë¸ ì‹œë„ ì¤‘: {model_name}")
                        
                        self.korean_llm = pipeline(
                            "text-generation",
                            model=model_name,
                            device=-1,  # CPU ì‚¬ìš©
                            framework="pt"
                        )
                        
                        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
                        test_result = self.korean_llm("ì•ˆë…•", max_length=50, num_return_sequences=1)
                        if test_result and len(test_result) > 0:
                            logger.info(f"âœ… {model_name} ëª¨ë¸ ë¡œë“œ ë° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                            self.current_korean_model = model_name
                            break
                        
                    except Exception as model_error:
                        logger.warning(f"âŒ {model_name} ì‹¤íŒ¨: {str(model_error)}")
                        continue
                
                if not self.korean_llm:
                    logger.error("ëª¨ë“  í•œêµ­ì–´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                    
            except Exception as e:
                logger.error(f"âŒ í•œêµ­ì–´ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
                self.korean_llm = None
        
        logger.info("ğŸ§  LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")
    
    async def generate_openai(self, prompt: str) -> Dict[str, Any]:
        """OpenAI GPTë¡œ ì‘ë‹µ ìƒì„±"""
        start_time = time.time()
        
        if not self.openai_client:
            return {
                "model": "openai_gpt",
                "text": "",
                "error": "OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ",
                "processing_time": 0
            }
        
        try:
            # í•œêµ­ì–´ ê³ ê°ì„¼í„° ë§¥ë½ ì¶”ê°€
            system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ì˜ ê¸ˆìœµ ê³ ê°ì„¼í„° ìƒë‹´ì›ì…ë‹ˆë‹¤. 
ê³ ê°ì˜ ë¬¸ì˜ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ê°„ê²°í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ í•´ì£¼ì„¸ìš”."""
            
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                self._openai_generate_sync,
                system_prompt, prompt
            )
            
            processing_time = time.time() - start_time
            
            return {
                "model": "openai_gpt",
                "text": response.choices[0].message.content.strip(),
                "tokens_used": response.usage.total_tokens,
                "processing_time": round(processing_time, 2),
                "finish_reason": response.choices[0].finish_reason
            }
            
        except Exception as e:
            logger.error(f"OpenAI ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return {
                "model": "openai_gpt",
                "text": "",
                "error": str(e),
                "processing_time": time.time() - start_time
            }
    
    def _openai_generate_sync(self, system_prompt: str, user_prompt: str):
        """OpenAI ë™ê¸° ì²˜ë¦¬"""
        return self.openai_client.chat.completions.create(
            model="gpt-3.5-turbo",  # ë˜ëŠ” "gpt-4"
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=500,
            temperature=0.7
        )
    
    async def generate_korean(self, prompt: str) -> Dict[str, Any]:
        """í•œêµ­ì–´ LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        start_time = time.time()
        
        if not self.korean_llm:
            return {
                "model": "korean_llm",
                "text": "",
                "error": "í•œêµ­ì–´ LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ",
                "processing_time": 0
            }
        
        try:
            # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                self._korean_llm_generate_sync,
                prompt
            )
            
            processing_time = time.time() - start_time
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì •ë¦¬
            if response and len(response) > 0:
                generated_text = response[0]["generated_text"]
                # ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ê³  ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                answer = generated_text.replace(prompt, "").strip()
                if not answer:  # ë‹µë³€ì´ ë¹„ì–´ìˆìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©
                    answer = generated_text.strip()
            else:
                answer = "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
            
            return {
                "model": "korean_llm",
                "text": answer,
                "processing_time": round(processing_time, 2),
                "full_response": response[0]["generated_text"] if response else "None"
            }
            
        except Exception as e:
            logger.error(f"í•œêµ­ì–´ LLM ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return {
                "model": "korean_llm",
                "text": "",
                "error": str(e),
                "processing_time": time.time() - start_time
            }
    
    def _korean_llm_generate_sync(self, prompt: str):
        """í•œêµ­ì–´ LLM ë™ê¸° ì²˜ë¦¬"""
        try:
            result = self.korean_llm(
                prompt,
                max_new_tokens=200,
                temperature=0.7,          
                do_sample=True,
                truncation=True
            )
            return result
        except Exception as e:
            logger.error(f"í•œêµ­ì–´ LLM ìƒì„± ì˜¤ë¥˜: {e}")
            # ë¹ˆ ì‘ë‹µ ë°˜í™˜ (ë”ë¯¸ ì—†ì´)
            return [{"generated_text": prompt}]
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("LLM ì„œë¹„ìŠ¤ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        
        if self.korean_model:
            del self.korean_model
            self.korean_model = None
            
        if self.korean_tokenizer:
            del self.korean_tokenizer
            self.korean_tokenizer = None
            
        if self.korean_llm:
            del self.korean_llm
            self.korean_llm = None
            
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ì‚¬ìš©í•˜ëŠ” ê²½ìš°)
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
        logger.info("âœ… LLM ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# ë¹„êµ í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜
async def compare_llm_models(prompt: str):
    """ë‘ LLM ëª¨ë¸ ë¹„êµ"""
    llm = LLMService()
    await llm.initialize()
    
    print(f"ğŸ§  í”„ë¡¬í”„íŠ¸: {prompt}")
    print("=" * 50)
    
    # ë³‘ë ¬ ì²˜ë¦¬
    tasks = [
        llm.generate_openai(prompt),
        llm.generate_korean(prompt)
    ]
    
    results = await asyncio.gather(*tasks)
    openai_result, korean_result = results
    
    print("ğŸŒ OpenAI GPT:")
    print(f"   ì‘ë‹µ: {openai_result.get('text', 'ERROR')}")
    print(f"   í† í°: {openai_result.get('tokens_used', 'N/A')}")
    print(f"   ì²˜ë¦¬ì‹œê°„: {openai_result.get('processing_time', 0)}ì´ˆ")
    
    print("\nğŸ‡°ğŸ‡· í•œêµ­ì–´ LLM:")
    print(f"   ì‘ë‹µ: {korean_result.get('text', 'ERROR')}")
    print(f"   ì²˜ë¦¬ì‹œê°„: {korean_result.get('processing_time', 0)}ì´ˆ")
    
    await llm.cleanup()
    return results

# í…ŒìŠ¤íŠ¸ìš©
async def test_llm_service():
    """LLM ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§  LLM ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    llm = LLMService()
    await llm.initialize()
    
    print("âœ… LLM ì´ˆê¸°í™” ì„±ê³µ!")
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
    test_prompts = [
        "ê³ ê° ë¬¸ì˜ì— ë‹µë³€í•´ì£¼ì„¸ìš”: ê³µì¸ì¸ì¦ì„œê°€ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê°±ì‹  ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”. ",
    ]
    
    for prompt in test_prompts:
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸: {prompt}")
        await compare_llm_models(prompt)
        print("-" * 30)
    
    await llm.cleanup()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_llm_service())